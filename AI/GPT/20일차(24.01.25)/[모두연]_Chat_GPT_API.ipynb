{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCn9RI5SuSSgA+bfN44TTb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Um-king/orm/blob/main/%5B%EB%AA%A8%EB%91%90%EC%97%B0%5D_Chat_GPT_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat GPT API를 활용한 실습 예제\n"
      ],
      "metadata": {
        "id": "0-Im-XPiCzmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# openai 모듈 설치\n",
        "\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3rvFaLKC32y",
        "outputId": "06fdf017-2f5b-42ec-c075-11107e310250"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.9.0-py3-none-any.whl (223 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.4/223.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.14)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions, h11, httpcore, httpx, openai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.9.0 typing-extensions-4.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI  # OpenAI 모듈 불러오기\n",
        "\n",
        "# OpenAI API 키 설정\n",
        "client = OpenAI(api_key='your-api-key')  # 사용자의 API 키로 대체해야 함\n",
        "\n",
        "# ChatGPT를 사용한 텍스트 생성 요청\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-3.5-turbo\",\n",
        "    messages = [{\"role\" : \"user\", \"content\" : \"Hello World!\"}]\n",
        ")\n",
        "# API 응답에서 마지막 메시지의 내용을 출력\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "# 나의 content를 보고 gpt 답변을 해줌 -> hello world에 대한 답을 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYZFO47IC5SF",
        "outputId": "ccdfe32b-1caa-4deb-cbdf-24079fa76f29"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 영어문장을 변역\n",
        "- 프랑스어\n",
        "- 한국어\n",
        "\n",
        "이걸 활용해서 번역 프로그램을 만들 수 있다."
      ],
      "metadata": {
        "id": "nZNdOThUP4uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# content에 입력한 영어문장을 프랑스어로 변경하는 예제\n",
        "\n",
        "import openai\n",
        "\n",
        "# ChatGPT를 사용한 텍스트 생성 요청\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-3.5-turbo\",\n",
        "    messages = [{\"role\" : \"user\", \"content\" : \"Translate the following English text to French: 'Hello, how are you?'\"}]\n",
        ")\n",
        "\n",
        "#응답 출력\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyppGzpPNJqR",
        "outputId": "dabcd207-e8e0-4cf6-9b4d-edaaf66babb0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bonjour, comment ça va ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# content에 입력한 영어문장을 한국어로 변경하는 예제\n",
        "\n",
        "import openai\n",
        "\n",
        "# ChatGPT를 사용한 텍스트 생성 요청\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-3.5-turbo\",\n",
        "    messages = [{\"role\" : \"user\", \"content\" : \"Translate the following English text to Korean: 'Hello, how are you?'\"}]\n",
        ")\n",
        "\n",
        "#응답 출력\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK9GQHkzPlVn",
        "outputId": "3bd1ac42-5810-4192-de97-df80071dcc49"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕하세요, 어떻게 지내세요?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10번을 수행하면서 시간에 따라 출력값의 차이가 있음을 확인 -> 이런 시간차를 이용해서 다양한 답을 얻을 수도 있다.\n",
        "# 돌리는 시간에 따라 조금씩 번역에 차이가 있음\n",
        "for i in range(10):\n",
        "    response = client.chat.completions.create(\n",
        "        model = \"gpt-3.5-turbo\",\n",
        "        messages = [{\"role\" : \"user\", \"content\" : \"Translate the following korean text to English: 안녕하세요 오늘 날씨가 좋네요. 햇살이 맑아요\"}],\n",
        "        max_tokens=10,\n",
        "        temperature=0.7,\n",
        "        top_p=0.8,\n",
        "        frequency_penalty=0.2\n",
        "    )\n",
        "    print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv8oi1MZP1YE",
        "outputId": "f3de1582-f305-4f22-f96a-1341f2f9c235"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, the weather is nice today. The sunlight\n",
            "Hello, today's weather is good. The sunlight\n",
            "Hello, the weather is nice today. The sunlight\n",
            "Hello, the weather is nice today. The sunlight\n",
            "Hello, today the weather is nice. The sunlight\n",
            "Hello, the weather is nice today. The sunlight\n",
            "Hello, the weather is nice today. The sunlight\n",
            "Hello, today's weather is nice. The sunlight\n",
            "Hello, today's weather is nice. The sunlight\n",
            "Hello, today the weather is nice. The sunlight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 블로그 글을 작성해주는 예시"
      ],
      "metadata": {
        "id": "p-UL_YaBQvun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_chatbot(messages):\n",
        "    response = client.chat.completions.create(\n",
        "        model = \"gpt-3.5-turbo\", messages = messages\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "prompt_role = \"너는 블로그 전문가, 파워블로그처럼 글을 써야해.\\\n",
        "                개발자의 직업관에 대한 글을 써야하고,\\\n",
        "                그리고 취업을 준비하는 20대 독자들에게 잘 보일수 있도록 글을 써야되\\\n",
        "                SEO최적화된 글을 써야되\""
      ],
      "metadata": {
        "id": "bIy5sqPWQQ62"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "def assist_blogger(\n",
        "    facts: List[str], tone: str, length_words: int, style: str\n",
        "):\n",
        "    facts = \", \".join(facts)\n",
        "    prompt_role = \"너는 블로그 전문가고, 파워블로그처럼 글을 써야해\" # 프롬프트 재정의!!! => 어떻게 써달라고 요청하는것 위의 변수 prompt_role 처럼 작성하는게 더 좋음\n",
        "    prompt = f\"{prompt_role} \\\n",
        "            FACTS: {facts} \\\n",
        "            TONE: {tone} \\\n",
        "            LEGNTH: {length_words} words \\\n",
        "            STYLE: {style}\"\n",
        "    return ask_chatbot([{\"role\": \"user\", \"content\": prompt}])"
      ],
      "metadata": {
        "id": "Y8ro1NvaQjmD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 블로그 포스트 내용 작성 -> [\"대학 진학 이후의 개발자의 삶은?\"]\n",
        "print(\n",
        "    assist_blogger(\n",
        "        [\"대학 진학 이후의 개발자의 삶은?\"], \"informal\", 100, \"blogpost\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m23ydseeQt6U",
        "outputId": "c3324d02-bacc-40b2-8a9f-4b6992bed35b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "있어! 그럼 지금부터 개발자들의 대학 진학 이후의 삶에 대해 이야기해볼게. 대학을 졸업하고 개발자로서 세상에 발을 내딛은 그 순간, 많은 사람들이 높은 기대를 갖게 되죠. 하지만 현실은 어떨까요? 대학에서 배웠던 이론과 실제 업무 사이에 차이가 있을 수도 있고, 업계에서 요구하는 기술들도 계속해서 변화하기 때문에 끊임없이 학습해야 해요. 개발자로 일하면서 계속해서 새로운 언어와 도구를 배워야하기 때문에 공부에 쏟는 시간도 많아지죠. 게다가 프로젝트의 마감 기한을 맞추기 위해 야근이나 주말 작업도 피할 수 없어요. 하지만! 이 모든 노고에도 불구하고 개발자로서의 성취감과 자기 만족감은 아주 훌륭한거에요. 새로운 아이디어를 현실화시키고 자신이 만든 제품이 많은 사람들에게 도움이 되는 것을 보면, 힘이 솟아나는 기분이 들거든요. 대학 진학 이후 개발자로서의 삶은 힘들기도 하지만, 동시에 보람찬 삶이기도 해요. 그러니까 지치지 말고 파워블로거처럼 열정적으로 글을 쓰는 나날을 보내봐요!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    assist_blogger(\n",
        "        facts =[\"Chat GPT 등장이후의 취업 전략은?\"], # 어떤 주제인지 제시\n",
        "        tone = \"정중하게\", # 글의 느낌\n",
        "        length_words = 200,\n",
        "        style= \"파워블로그 스럽게\" # 블로그 스타일\n",
        "        )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v77oQ9iSUmm",
        "outputId": "98015626-53f0-4ef2-e618-f539cd4bb027"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕하세요! 저는 블로그 전문가입니다. 오늘은 Chat GPT 등장 이후의 취업 전략에 대해 이야기해 보려고 합니다.\n",
            "\n",
            "Chat GPT는 인공지능 기술의 발전으로 자연어 처리(NLP) 분야에서 큰 성과를 거둔 모델입니다. 이 모델은 실제로 상당한 영향력을 가지고 있으며, 기업들이 Chat GPT를 활용하여 채용 프로세스를 개선하고 일상적인 업무를 자동화하기도 합니다.\n",
            "\n",
            "Chat GPT 등장 이후의 취업 전략을 구체적으로 설명해보자면, 우선적으로 자신의 역량과 경험을 강조해야 합니다. 인공지능 기술의 발전은 업계에서 새로운 기회를 제공하기는 하지만, 결국 사람이 중심이 되는 것은 변함이 없습니다. 자신이 어떠한 분야에서 뛰어나고 독특한 경험을 갖고 있다는 것을 강조하는 것이 중요합니다.\n",
            "\n",
            "또한, 채용 프로세스의 변화에 맞춰 블로그 작성 경험을 강조해야 합니다. Chat GPT를 사용하면서 핵심 역량인 커뮤니케이션과 문서 작성 능력 역시 중요시되기 때문에, 자신의 블로그 경험을 어필하는 것이 도움이 될 수 있습니다. 특히 파워블로그 스럽게 글을 쓴 경험을 갖고 있다면, 이를 적극적으로 활용해 주세요.\n",
            "\n",
            "마지막으로, 정중하고 전문적인 태도를 유지하며 채용 과정에 진입해야 합니다. Chat GPT 등장 이후에도 인간적인 존재로서의 가치는 변함이 없습니다. 실제로 인터뷰에서 자신을 잘 어필하고, 상세한 경험과 역량을 설명하는 것이 큰 도움이 될 것입니다.\n",
            "\n",
            "이상, Chat GPT 등장 이후의 취업 전략에 대해 알아보았습니다. 감사합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 유튜브 번역"
      ],
      "metadata": {
        "id": "iumn-BT9TkRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/English_But_what_is_a_neural_network____Chapter_1_Deep_learning_DownSub.com.txt\", \"r\") as f:\n",
        "    transcript = f.read()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-3.5-turbo\",\n",
        "    messages = [\n",
        "        {\"role\" : \"system\", \"content\" : \"너는 유튜브를 영어에서 한국어로 번역하는 번역가이자, 요약을 잘하는 역할을 할꺼야\"},\n",
        "        {\"role\" : \"user\", \"content\" : \"업로드한 파일을 한국어로 변역해줘\"},\n",
        "        {\"role\" : \"assistant\", \"content\" : \"Yes.\"},\n",
        "        {\"role\" : \"user\", \"content\" : \"한국어로 번역한 내용을 요약해\"},\n",
        "        {\"role\" : \"assistant\", \"content\" : \"Yes.\"},\n",
        "        {\"role\" : \"user\", \"content\" : transcript}\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "# gpt 3.5버전에서는 길이가 한도가 있음 -> 4097 tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "rq_avc1fTMHA",
        "outputId": "daaeab7d-b24f-486c-d1c0-b0e4f0343cee"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4119 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-6f81e7714029>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtranscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     messages = [\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 648\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    649\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1177\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         )\n\u001b[0;32m-> 1179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 868\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    869\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4119 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 짧게 짧게 나눠서 출력\n",
        "\n",
        "# 첨부된 파일을 읽고 작은 부분으로 나누기 위한 코드입니다.\n",
        "\n",
        "# 파일 경로 설정\n",
        "file_path = '/content/English_But_what_is_a_neural_network____Chapter_1_Deep_learning_DownSub.com.txt'\n",
        "\n",
        "# 파일을 읽어서 내용을 저장\n",
        "with open(file_path, 'r') as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# 텍스트를 나눌 최대 길이 설정 (토큰 수가 아닌 문자 수 기준)\n",
        "max_length = 5000  # 각 부분의 최대 길이 (문자 수)\n",
        "\n",
        "# 텍스트를 작은 부분으로 나누는 함수\n",
        "def split_into_parts(text, length):\n",
        "    return [text[i:i+length] for i in range(0, len(text), length)]\n",
        "\n",
        "# 텍스트를 여러 부분으로 나눔\n",
        "parts = split_into_parts(transcript, max_length)\n",
        "\n",
        "# 나누어진 부분들의 수와 첫 부분의 내용 일부를 출력\n",
        "num_parts = len(parts)\n",
        "first_part_preview = parts[0][:500]  # 첫 부분의 처음 500자\n",
        "\n",
        "num_parts, first_part_preview"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twEfUud1URca",
        "outputId": "0c197e1f-4252-4302-cde2-97f41193e573"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,\n",
              " \"This is a 3. It's sloppily written and rendered at an extremely low resolution of 28x28 pixels,\\n\\nbut your brain has no trouble recognizing it as a 3. And I want you to take a moment\\n\\nto appreciate how crazy it is that brains can do this so effortlessly. I mean, this,\\n\\nthis and this are also recognizable as 3s, even though the specific values of each pixel\\n\\nis very different from one image to the next. The particular light-sensitive cells in your\\n\\neye that are firing when you see this 3 are very \")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어로 번역하고 요약\n",
        "\n",
        "# 첫 번째 부분만 사용\n",
        "first_part = parts[0]\n",
        "\n",
        "# 첫 번째 부분에 대한 번역 요청\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages = [\n",
        "        {\"role\" : \"system\", \"content\" : \"너는 유튜브를 영어에서 한국어로 번역하는 번역가이자, 요약을 잘하는 역할을 할꺼야\"},\n",
        "        {\"role\" : \"user\", \"content\" : \"업로드한 파일을 한국어로 변역해줘\"},\n",
        "        {\"role\" : \"assistant\", \"content\" : \"Yes.\"},\n",
        "        {\"role\" : \"user\", \"content\" : \"한국어로 번역한 내용을 요약해\"},\n",
        "        {\"role\" : \"assistant\", \"content\" : \"Yes.\"},\n",
        "        {\"role\" : \"user\", \"content\" : first_part}\n",
        "      ],\n",
        ")\n",
        "\n",
        "# 번역 결과 출력\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-dfeEn4Uivz",
        "outputId": "5579ef3e-ab6f-4a0b-f658-80af68da00cd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이것은 3입니다. 손글씨로 놓여진 것이며, 해상도는 극히 낮은 28x28 픽셀입니다. 하지만 당신의 뇌는 이것을 3으로 쉽게 인식할 수 있습니다. 심지어 이것과 이것도 3으로 인식될 수 있지만, 각 픽셀의 구체적인 값은 이미지마다 매우 다릅니다. 당신이 이 3을 볼 때 활성화되는 눈에 특정한 빛에 민감한 세포는 이 3을 볼 때 활성화되는 세포와 매우 다릅니다. 그렇지만 당신의 비정상적으로 똑똑한 시각 피질의 어딘가에서는 동일한 개념을 나타낸다고 인식합니다. 동시에, 다른 이미지는 별개의 개념으로 인식됩니다. 하지만 픽셀이 28x28 그리드를 받아들여 숫자 0에서 10 사이의 하나의 숫자를 출력하는 프로그램을 작성하라고 하면, 일컫는 검증적으로 처지게 됩니다. 돌 뒤에 살고 있다면, 기계 학습과 신경망의 관련성과 중요성을 설명할 필요조차 없을 것입니다. 하지만 여기에서 제공하려는 것은 실제로 신경망이 무엇인지를 보여주는 것입니다. 배경 지식이 전혀 없다고 가정하고 이것이 무엇을 의미하는지, 신경망이 '학습'을 읽거나 들을 때 이것이 무엇을 의미하는지 알 수 있도록 시각화하는 데 도움이 되기를 바랍니다. 이 비디오는 그 구조에만 집중할 것이며, 다음 비디오는 학습에 대해 다룰 것입니다. 우리는 손으로 쓴 숫자를 인식할 수 있는 신경망을 만들 것입니다. 이것은 주제를 소개하기 위해 어느 정도 고전적인 예제입니다. 비디오가 끝나면 독자적으로 작업을 할 수 있는 우수한 리소스 몇 군데와 코드를 다운로드하여 직접 컴퓨터에서 작업할 수 있는 곳을 가르켜 드릴 것입니다. 여러 가지 변형 된 신경망이 있으며, 최근 몇 년간 이러한 변형에 대한 연구가 급증했습니다. 하지만 이 두 가지 간단한 소개 비디오에서는 추가적인 것 없이 가장 간단하고 베이스 모델만 살펴보려고 합니다. 이것은 더욱 강력한 현대적인 변형을 이해하기 위한 필수 선행 조건이며, 그러나 여전히 우리가 이해하기에 충분한 복잡성을 가지고 있습니다.가장 단순한 형태에서라도 컴퓨터가 손으로 쓴 숫자를 인식할 수 있다는 것은 정말 멋진 일입니다. 동시에 우리가 갖는 희망에는 아직까지 충분하게 만족시킬 수 없다는 것을 알게 될 것입니다. 이름에서 알 수 있듯이 신경망은 뇌에서 영감을 받지만, 뇌에서 어떻게 그룹화 된 신경 세포의 발화가 다른 신경세포의 발화를 유발하는 데 관련되어 있는지 알아보겠습니다. 현재로서만 뉴런이라고 말할 때, 0과 1 사이의 숫자를 저장하는 것에 대해 생각하면 됩니다. 예를 들어, 네트워크는 입력 이미지의 각 28x28 픽셀에 해당하는 뉴런들로 시작합니다. 이는 총 784개의 뉴런입니다. 이들 각각은 해당 픽셀의 그레이스케일 값을 나타내는 숫자를 보유합니다. 0은 검은 픽셀, 1은 흰 픽셀을 나타냅니다. 이 뉴런 안에 있는 숫자를 화성화 라고하며, 뇌를 생각해볼 때 숫자가 높을수록 뉴런이 켜진다고 생각하시면 됩니다. 이 784개의 뉴런은 네트워크의 첫 번째 레이어를 이루며, 마지막 레이어에서 10개의 뉴런이 각각 10 개의 숫자를 나타냅니다. 이 뉴런들의 활성화는 얼마나 시스템이 특정 이미지가 특정 숫자와 일치한다고 생각하는지를 나타냅니다. 가운데에는 미지의 레이어들이 있으며, 우리는 현재 이 레이어를 숫자를 인식하는 방식을 어떻게 처리해야 할지에 대한 큰 의문으로 두려워해야합니다. 본 신경망에서는 2개의 은닉층을 선택했으며, 각각 16개의 뉴런을 가지고 있습니다. 솔직히 말하면, 이는 구조를 얼마나 설득력 있게 제시할지를 위해 선택한 임의의 값입니다. 실제로는 여기에서 특정 구조로 실험할 수 있는 많은 여지가 있습니다. 네트워크는 한 레이어의 활성화가 다음 레이어의 활성화를 결정합니다. 그리고 물론 정보 처리 메커니즘으로서의 신경망의 핵심은 한 레이어의 활성화가 다음 레이어의 활성화에 어떻게 영향을 주는지에 달려 있습니다. 이는 생물학적 신경 세포의 네트워크에서 어떤 그룹의 신경세포가 특정 그룹을 활성화시키는지와 유사하다고 볼 수 있습니다. 여기에 표시된 네트워크는 이미 숫자를 인식하는 데에 훈련되었으며, 보여드리겠습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 부분에 대한 번역 및 요약 결과를 저장할 리스트\n",
        "translated_summaries = []\n",
        "\n",
        "# 모든 부분에 대해 번역 및 요약 요청 수행\n",
        "for part in parts:\n",
        "    # 실제 환경에서는 이 부분에 API 요청을 넣어야 합니다.\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages = [\n",
        "        {\"role\" : \"system\", \"content\" : \"너는 유튜브를 영어에서 한국어로 번역하는 번역가이자, 요약을 잘하는 역할을 할꺼야\"},\n",
        "        {\"role\" : \"user\", \"content\" : \"업로드한 파일을 한국어로 변역해줘\"},\n",
        "        {\"role\" : \"assistant\", \"content\" : \"Yes.\"},\n",
        "        {\"role\" : \"user\", \"content\" : \"한국어로 번역한 내용을 요약해\"},\n",
        "        {\"role\" : \"assistant\", \"content\" : \"Yes.\"},\n",
        "        {\"role\" : \"user\", \"content\" : part}\n",
        "      ],\n",
        "    )\n",
        "    # 번역 및 요약 결과 저장\n",
        "    translated_summary = response.choices[0].message.content\n",
        "    translated_summaries.append(translated_summary)\n",
        "\n",
        "# 모든 결과를 하나의 문자열로 결합\n",
        "final_result = '\\n\\n'.join(translated_summaries)\n",
        "\n",
        "# 최종 결과 출력\n",
        "print(final_result)"
      ],
      "metadata": {
        "id": "RU5kC4fWU_TQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}